{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizing words and sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Abhijeet\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk   #Package for NLP\n",
    "import numpy as np\n",
    "nltk.download('punkt')   #It contains other dictionaries for stemming and other tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraph =\"\"\"Thank you all so very much. Thank you to everybody at \n",
    "               Fox and New Regency … my entire team. I have to thank \n",
    "               everyone from the very onset of my career … To my parents; \n",
    "               none of this would be possible without you. And to my \n",
    "               friends, I love you dearly; you know who you are. And lastly,\n",
    "               I just want to say this: Making The Revenant was about\n",
    "               man's relationship to the natural world. A world that we\n",
    "               collectively felt in 2015 as the hottest year in recorded\n",
    "               history. Our production needed to move to the southern\n",
    "               tip of this planet just to be able to find snow.  We need to\n",
    "               support leaders around the world who do not speak for the \n",
    "               big polluters, but who speak for all of humanity, for the\n",
    "               indigenous people of the world, for the billions and \n",
    "               billions of underprivileged people out there who would be\n",
    "               most affected by this. I thank you all for this \n",
    "               amazing award tonight. Let us not take this planet for \n",
    "               granted. I do not take tonight for granted. Thank you so very much.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SentencesTokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = nltk.sent_tokenize(paragraph) # Sentence tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Thank you all so very much.',\n",
       " 'Thank you to everybody at \\n               Fox and New Regency … my entire team.',\n",
       " 'I have to thank \\n               everyone from the very onset of my career … To my parents; \\n               none of this would be possible without you.',\n",
       " 'And to my \\n               friends, I love you dearly; you know who you are.',\n",
       " \"And lastly,\\n               I just want to say this: Making The Revenant was about\\n               man's relationship to the natural world.\",\n",
       " 'A world that we\\n               collectively felt in 2015 as the hottest year in recorded\\n               history.',\n",
       " 'Our production needed to move to the southern\\n               tip of this planet just to be able to find snow.',\n",
       " 'We need to\\n               support leaders around the world who do not speak for the \\n               big polluters, but who speak for all of humanity, for the\\n               indigenous people of the world, for the billions and \\n               billions of underprivileged people out there who would be\\n               most affected by this.',\n",
       " 'I thank you all for this \\n               amazing award tonight.',\n",
       " 'Let us not take this planet for \\n               granted.',\n",
       " 'I do not take tonight for granted.',\n",
       " 'Thank you so very much.']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences #See sentences are tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = nltk.word_tokenize(paragraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Thank',\n",
       " 'you',\n",
       " 'all',\n",
       " 'so',\n",
       " 'very',\n",
       " 'much',\n",
       " '.',\n",
       " 'Thank',\n",
       " 'you',\n",
       " 'to',\n",
       " 'everybody',\n",
       " 'at',\n",
       " 'Fox',\n",
       " 'and',\n",
       " 'New',\n",
       " 'Regency',\n",
       " '…',\n",
       " 'my',\n",
       " 'entire',\n",
       " 'team',\n",
       " '.',\n",
       " 'I',\n",
       " 'have',\n",
       " 'to',\n",
       " 'thank',\n",
       " 'everyone',\n",
       " 'from',\n",
       " 'the',\n",
       " 'very',\n",
       " 'onset',\n",
       " 'of',\n",
       " 'my',\n",
       " 'career',\n",
       " '…',\n",
       " 'To',\n",
       " 'my',\n",
       " 'parents',\n",
       " ';',\n",
       " 'none',\n",
       " 'of',\n",
       " 'this',\n",
       " 'would',\n",
       " 'be',\n",
       " 'possible',\n",
       " 'without',\n",
       " 'you',\n",
       " '.',\n",
       " 'And',\n",
       " 'to',\n",
       " 'my',\n",
       " 'friends',\n",
       " ',',\n",
       " 'I',\n",
       " 'love',\n",
       " 'you',\n",
       " 'dearly',\n",
       " ';',\n",
       " 'you',\n",
       " 'know',\n",
       " 'who',\n",
       " 'you',\n",
       " 'are',\n",
       " '.',\n",
       " 'And',\n",
       " 'lastly',\n",
       " ',',\n",
       " 'I',\n",
       " 'just',\n",
       " 'want',\n",
       " 'to',\n",
       " 'say',\n",
       " 'this',\n",
       " ':',\n",
       " 'Making',\n",
       " 'The',\n",
       " 'Revenant',\n",
       " 'was',\n",
       " 'about',\n",
       " 'man',\n",
       " \"'s\",\n",
       " 'relationship',\n",
       " 'to',\n",
       " 'the',\n",
       " 'natural',\n",
       " 'world',\n",
       " '.',\n",
       " 'A',\n",
       " 'world',\n",
       " 'that',\n",
       " 'we',\n",
       " 'collectively',\n",
       " 'felt',\n",
       " 'in',\n",
       " '2015',\n",
       " 'as',\n",
       " 'the',\n",
       " 'hottest',\n",
       " 'year',\n",
       " 'in',\n",
       " 'recorded',\n",
       " 'history',\n",
       " '.',\n",
       " 'Our',\n",
       " 'production',\n",
       " 'needed',\n",
       " 'to',\n",
       " 'move',\n",
       " 'to',\n",
       " 'the',\n",
       " 'southern',\n",
       " 'tip',\n",
       " 'of',\n",
       " 'this',\n",
       " 'planet',\n",
       " 'just',\n",
       " 'to',\n",
       " 'be',\n",
       " 'able',\n",
       " 'to',\n",
       " 'find',\n",
       " 'snow',\n",
       " '.',\n",
       " 'We',\n",
       " 'need',\n",
       " 'to',\n",
       " 'support',\n",
       " 'leaders',\n",
       " 'around',\n",
       " 'the',\n",
       " 'world',\n",
       " 'who',\n",
       " 'do',\n",
       " 'not',\n",
       " 'speak',\n",
       " 'for',\n",
       " 'the',\n",
       " 'big',\n",
       " 'polluters',\n",
       " ',',\n",
       " 'but',\n",
       " 'who',\n",
       " 'speak',\n",
       " 'for',\n",
       " 'all',\n",
       " 'of',\n",
       " 'humanity',\n",
       " ',',\n",
       " 'for',\n",
       " 'the',\n",
       " 'indigenous',\n",
       " 'people',\n",
       " 'of',\n",
       " 'the',\n",
       " 'world',\n",
       " ',',\n",
       " 'for',\n",
       " 'the',\n",
       " 'billions',\n",
       " 'and',\n",
       " 'billions',\n",
       " 'of',\n",
       " 'underprivileged',\n",
       " 'people',\n",
       " 'out',\n",
       " 'there',\n",
       " 'who',\n",
       " 'would',\n",
       " 'be',\n",
       " 'most',\n",
       " 'affected',\n",
       " 'by',\n",
       " 'this',\n",
       " '.',\n",
       " 'I',\n",
       " 'thank',\n",
       " 'you',\n",
       " 'all',\n",
       " 'for',\n",
       " 'this',\n",
       " 'amazing',\n",
       " 'award',\n",
       " 'tonight',\n",
       " '.',\n",
       " 'Let',\n",
       " 'us',\n",
       " 'not',\n",
       " 'take',\n",
       " 'this',\n",
       " 'planet',\n",
       " 'for',\n",
       " 'granted',\n",
       " '.',\n",
       " 'I',\n",
       " 'do',\n",
       " 'not',\n",
       " 'take',\n",
       " 'tonight',\n",
       " 'for',\n",
       " 'granted',\n",
       " '.',\n",
       " 'Thank',\n",
       " 'you',\n",
       " 'so',\n",
       " 'very',\n",
       " 'much',\n",
       " '.']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chatbot",
   "language": "python",
   "name": "chatbot"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
